<title>incast</title></head>
<table width="700">
<tr><td>
Tom Hutton from UCSD has suggested 
<a href="https://web.archive.org/web/20260106010757/https://people.ucsc.edu/~warner/Bufs/understanding-tcp-incast.pdf">
Understanding TCP Incast Throughput Collapse in Datacenter Networks</a> [2009]
as a good tutorial on <i>incast</i>.
<p>
In RFC 6298 (June 2011) the initial TCP retransmission time out (RTO) was made 
shorter. The initial RTO value is used when TCP starts until a measured 
RTT can be derived frm the stream. The previous value was 3 seconds; the 
new recommendation is 1 second.  This makes people that use TCP in datacenters
for short RTT flows feel misunderstood and unappreciated. 
<p>
Within a data center, an adjusted initial RTO was shown by the authors of <i>Understanding...</i> 
to go a long way toward addressing the ills caused by 
incast. Clearly, ultra-short initial RTOs would <b> not</b> be a good idea for 
science data set transfer over WAN distances. The incast problem is that the
nature of the work (<i>e.g.</i> MapReduce, distributed file system) causes client 
responders to synchronize their messages which in turn creates an overload 
on the link carrying results back to the server querier.
<p>
<i>Understanding. . .</i> did not seriously consider large buffers as a solution to
<i>incast</i>. They said:
<blockquote>
The authors also attempted a variety of non-TCP work-arounds
and identified the shortcomings of each. Increasing the size
of switch and router buffers delays the onset problem to configurations
with more concurrent senders. However, switches
and routers with large buffers are expensive, and even large
buffers may be filled up quickly with ever higher speed links.
</blockquote>

<i>Understanding</i> used <a href="https://web.archive.org/web/20260106010757/https://people.ucsc.edu/~warner/Bufs/nortel5500"> Nortel 5500</a> switches for their experimental
work.  These switches are included in the 
<a href="https://web.archive.org/web/20260106010757/http://people.ucsc.edu/~warner/buffer.html"> buffer tabulation table.</a>
An important point is that mega-datacenters have <i>voted</i> with their
purchases and merchant silicon switch chips with integrated buffers
have won the TOR beauty contest. One can only conclude that while the
Kilobytes in older switches were insuffient, the Megabytes in current
SoC switches solve the problem.
<p>
Broadcom addressed the data center situation in
<a href="https://web.archive.org/web/20260106010757/https://people.ucsc.edu/~warner/Bufs/SBT-ETP100.pdf">
Broadcom Smart-Buffer Technology in Data Center Switches</a> in April
2012. As commercial white papers go, this one is less painful than most.
<i>n.b.</i> Written from the perspective of the day that dynamic buffers
were a Broadcom innovation that provided a competitive advantage over
other chip manufacturers. That is no longer the case.
<p>
That the authors of <i> Understanding</i> reject really large buffers [&gt;1000 MB] as a
solution to this problem, this has clearly not stopped vendors from offering
them.
<p>
Cisco put together a 2011 white paper that covers <a href="https://web.archive.org/web/20260106010757/https://people.ucsc.edu/~warner/Bufs/white_paper_c11-690561.pdf">
networking for big data</a> that shows buffer use at stages of <i>map/reduce</i>.
</td></tr>
</table>