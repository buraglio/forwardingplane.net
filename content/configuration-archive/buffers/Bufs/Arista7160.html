<table width="700">
<tr><td>
<img src="https://web.archive.org/web/20260106010555im_/https://people.ucsc.edu/~warner/Bufs/7160.png">
</td></tr>
<tr><td>
Introduced December 5 2016. The Cavium Xpliant switch silicon is a single core design
with 24 MB of packet memory in a single pool. Arista has put out an
<a href="https://web.archive.org/web/20260106010555/https://people.ucsc.edu/~warner/Bufs/7160SwitchArchitecture_WP.pdf">architecture paper.</a> The three variants above
represent ways that 128 SERDES can be distributed across the front panel ports that will
fit in a 1 RU form factor. At the top of the heap, the <i>48Y</i> has 48 SFP28 ports and
6 QSFP28 100 Gb/s ports.
The <i>48T</i> has 48 10 G-base-T ports and 6 QSFP28 100 Gb/s ports. The <i>32CQ</i> has 32 QSFP28 ports. 
<p>
<ul>
<b>Shared Buffer Architecture</b> <i>Lifted from the paper cited above</i>
<p>
The 7160 Series incorporates an advanced traffic manager with 24MB of packet buffer that is fully shared across all ports and is an
excellent choice for scalable datacenters and modern intensive workloads. Unlike architectures where the total buffer is statically
allocated to a port or group of ports or the packet memory or buffers are formed of multiple slices, the 7160 Series packet buffer is
dynamically allocated across all ports with the ability to adjust in real time to the demands of bursty applications, mixed interface
speeds and congestion.
<p>
To optimize the memory performance, the 24MB is divided into 96K pages of 256 byte cells, organized in 16 banks each of 1.5MB.
This ensures an even distribution of page usage over the total memory. A cell can contain a single packet, or a linked list of 256 byte
cells is used to buffer packets larger than a single cell.
The packet buffer is carved by software into multiple shared pools to handle high priority traffic and Priority Flow Control (PFC)
enabled ports separately from the other traffic. This ensures the switch packet buffer is optimally allocated for the specific use case.s
unique requirements.
<p>
Extensive support for Active Queue Management mechanisms such as WRED, DCTCP and ECN ensure that both high priority flows
and lossless storage traffic are handled equally well with the ability to absorb large bursts with extensive counters for visibility and
accounting.
</ul>
According to the data sheet:
<p>
<ul>
Latency Analyzer and Microburst Detection (LANZ)
<ul>
<li> Configurable Congestion Notification (CLI, Syslog) *
<li> Streaming Events (GPB Encoded) *
<li> Capture/Mirror of congested traffic *
</ul>
<p>
* Not currently supported in EOS
</ul>
</ul>

</td></tr>