<!DOCTYPE html>
<html lang="en-us">

<head>
<meta charset="utf-8" />
<meta name="author" content="Nick Buraglio" />
<meta name="description" content="A wealth of run on sentences, misspelled words, and grammatical errors related to Routing, Switching, Security, Strategy…." />
<meta name="keywords" content="blog, tech" />
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.58.3" />

<link rel="canonical" href="https://forwardingplane.net/post/identify-and-remedy-problem-ike-and-eventd-processes-on-juniper-srx/">
<meta property="og:title" content="Identify and remedy problem IKE and eventd processes on Juniper SRX" />
<meta property="og:description" content="Recently we encountered a very strange behavior on an SRX 5800 cluster. The cluster, which is in active/active mode, started dropping OSPF adjacencies to it&rsquo;s neighboring routing equipment, in this case, Juniper MX480 and Brocade/Foundry MLX8. Strange behavior indeed, since for us, these had been rock solid for around 2 years and we&rsquo;d never seen this odd behavior before. Honestly, we started looking at the routers first since this was something the SRX has never done before." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://forwardingplane.net/post/identify-and-remedy-problem-ike-and-eventd-processes-on-juniper-srx/" />
<meta property="article:published_time" content="2013-02-04T12:45:18+00:00" />
<meta property="article:modified_time" content="2013-02-04T12:45:18+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Identify and remedy problem IKE and eventd processes on Juniper SRX"/>
<meta name="twitter:description" content="Recently we encountered a very strange behavior on an SRX 5800 cluster. The cluster, which is in active/active mode, started dropping OSPF adjacencies to it&rsquo;s neighboring routing equipment, in this case, Juniper MX480 and Brocade/Foundry MLX8. Strange behavior indeed, since for us, these had been rock solid for around 2 years and we&rsquo;d never seen this odd behavior before. Honestly, we started looking at the routers first since this was something the SRX has never done before."/>


<meta itemprop="name" content="Identify and remedy problem IKE and eventd processes on Juniper SRX">
<meta itemprop="description" content="Recently we encountered a very strange behavior on an SRX 5800 cluster. The cluster, which is in active/active mode, started dropping OSPF adjacencies to it&rsquo;s neighboring routing equipment, in this case, Juniper MX480 and Brocade/Foundry MLX8. Strange behavior indeed, since for us, these had been rock solid for around 2 years and we&rsquo;d never seen this odd behavior before. Honestly, we started looking at the routers first since this was something the SRX has never done before.">


<meta itemprop="datePublished" content="2013-02-04T12:45:18&#43;00:00" />
<meta itemprop="dateModified" content="2013-02-04T12:45:18&#43;00:00" />
<meta itemprop="wordCount" content="988">



<meta itemprop="keywords" content="Routing,Security," />


<link rel="stylesheet" href="https://forwardingplane.net/css/layout.css" />


<link rel="stylesheet" href="https://forwardingplane.net/css/default-dark.css" />




<title>


     Identify and remedy problem IKE and eventd processes on Juniper SRX 

</title>

</head>


<body>
<div class="main">
<header>

<div class="header-bar">

  <nav>
    <div class="siteTitle">
      <a href="https://forwardingplane.net/">ForwardingPlane.net</a>
    </div> 

    
    
    <a class="nav-item" href="https://forwardingplane.net/post/"><div class="nav-item-title">Posts</div></a>
    
    <a class="nav-item" href="https://forwardingplane.net/tags/"><div class="nav-item-title">Tags</div></a>
    
    <a class="nav-item" href="https://forwardingplane.net/about"><div class="nav-item-title">about</div></a>
    
    <a class="nav-item" href="https://forwardingplane.net/configuration-archive"><div class="nav-item-title">configuration-archive</div></a>
    

  </nav>

  
<div class="social-links-header">

  
  <a href="mailto:my%20lastname%20at%20forwardingplane%20dot%20net"><div class="social-link">email</div></a>
  

  
  <a href="https://github.com/buraglio" target="_blank"><div class="social-link">gh</div></a>
  

  

  
  <a href="https://twitter.com/forwardingplane" target="_blank"><div class="social-link">twtr</div></a>
  

  
  <a href="https://www.linkedin.com/in/buraglio" target="_blank"><div class="social-link">in</div></a>
  

</div>


</div>


</header>


<article class="post">
    <h1 class="title"> Identify and remedy problem IKE and eventd processes on Juniper SRX </h1>
    <div class="content"> <p>Recently we encountered a very strange behavior on an SRX 5800 cluster.  The cluster, which is in active/active mode, started dropping OSPF adjacencies to it&rsquo;s neighboring routing equipment, in this case, Juniper MX480 and Brocade/Foundry MLX8.  Strange behavior indeed, since for us, these had been rock solid for around 2 years and we&rsquo;d never seen this odd behavior before. Honestly, we started looking at the routers first since this was something the SRX has never done before.  After noticing that it was actually link dropping and not just OSPF having issues, we dug deeper into logs (as an aside, this is an <strong>excellent</strong> reason to centrally syslog everything.  And I do mean <strong>everything</strong>).  To our surprise and dismay, it was actually the SRX node0 actually rebooting all of its interface line cards. &ldquo;<em>show chassis routing-engine</em>&rdquo; showed that User was taking up a very significant amount of CPU.  This is a problem.</p>

<p><a href="http://www.forwardingplane.net/wp-content/uploads/2013/02/Screen-Shot-2013-02-01-at-10.37.27-AM.png"><img src="http://www.forwardingplane.net/wp-content/uploads/2013/02/Screen-Shot-2013-02-01-at-10.37.27-AM.png" alt="Screen Shot 2013-02-01 at 10.37.27 AM" /></a></p>

<p>  As you can probably guess, this isn&rsquo;t a good state.  So, in order to drill down what was causing User to be so abnormally high, we had to do a little sleuthing.  Running &ldquo;<em>show system processes extensive | except 0.00</em>&rdquo; will display any process that isn&rsquo;t zero.  From here it was pretty obvious what was eating the CPU.</p>

<p><a href="http://www.forwardingplane.net/wp-content/uploads/2013/02/Screen-Shot-2013-02-01-at-10.43.50-AM.png"><img src="http://www.forwardingplane.net/wp-content/uploads/2013/02/Screen-Shot-2013-02-01-at-10.43.50-AM.png" alt="Screen Shot 2013-02-01 at 10.43.50 AM" /></a></p>

<p>  &ldquo;kmd&rdquo; and &ldquo;eventd&rdquo;, as you can see, are chewing up an abnormally high amount of CPU.  First things first, make sure no traceoptions are on. &ldquo;<em>show conf | display set | match traceoptions</em>&rdquo;  We had some on so we disabled them.  Next, lets make sure we know what &ldquo;kid&rdquo; and &ldquo;eventd&rdquo; are. <a href="http://www.juniper.net/techpubs/software/junos/junos94/syslog-messages/kmd-system-log-messages.html">KMD</a> is the key management process. It provides IP Security (IPSec) authentication services for encryption Physical Interface Cards (PICs). <a href="http://www.juniper.net/techpubs/en_US/junos10.1/information-products/topic-collections/syslog-messages/jd0e22130.html">eventd</a> is the event policy process. It performs configured actions in response to events on a routing platform that trigger system log messages. It&rsquo;s all starting to add up.  I&rsquo;ll bet there are a LOT of IPsec messeges in the log. &ldquo;<em>show log messages</em>&rdquo; confirms this.  There are a LOT of KMD messages, which is likely causing events to eat even more resources to generate them. <em>Jan 29 12:02:53 fw1 (FPC Slot 11, PIC Slot 0) init: kmd1 (PID 176) started</em> _Jan 29 12:02:53 _fw1<em> (FPC Slot 11, PIC Slot 0) init: kmd2 (PID 177) started</em> _Jan 29 12:02:53 _fw1<em> (FPC Slot 11, PIC Slot 0) init: kmd3 (PID 178) started</em> _Jan 29 12:02:53 _fw1<em> (FPC Slot 11, PIC Slot 0) init: kmd4 (PID 179) started</em> _Jan 29 12:02:54 _fw1<em> (FPC Slot 11, PIC Slot 1) init: kmd1 (PID 176) started</em> _Jan 29 12:02:55 _fw1<em> (FPC Slot 11, PIC Slot 1) init: kmd2 (PID 177) started</em> _Jan 29 12:02:55 _fw1<em> (FPC Slot 11, PIC Slot 1) init: kmd3 (PID 178) started</em> _Jan 29 12:02:55 _fw1<em> (FPC Slot 11, PIC Slot 1) init: kmd4 (PID 180) started</em> _Jan 29 12:03:13 _fw1<em> (FPC Slot 4, PIC Slot 0) init: kmd1 (PID 176) started</em> _Jan 29 12:03:13 _fw1<em> (FPC Slot 4, PIC Slot 0) init: kmd2 (PID 177) started</em> _Jan 29 12:03:13 _fw1<em> (FPC Slot 4, PIC Slot 0) init: kmd3 (PID 178) started</em> _Jan 29 12:03:13 _fw1<em> (FPC Slot 4, PIC Slot 0) init: kmd4 (PID 179) started</em> _Jan 29 12:03:14 _fw1<em> (FPC Slot 5, PIC Slot 0) init: kmd1 (PID 176) started</em> _Jan 29 12:03:14 _fw1<em> (FPC Slot 5, PIC Slot 0) init: kmd2 (PID 177) started</em> _Jan 29 12:03:14 _fw1<em> (FPC Slot 5, PIC Slot 0) init: kmd3 (PID 178) started</em> _Jan 29 12:03:15 _fw1<em> (FPC Slot 5, PIC Slot 0) init: kmd4 (PID 179) started</em>   Check the security log to verify &ldquo;<em>show log kmed</em>&rdquo; <em>Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_ipc_err_handler: status: 1: usp_ipc_client_open: failed to connect to the server after 5 retries Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_send_msg_to_spu: failed to connect to iked_spu on SPU - Connection refused. Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_ipc_err_handler: status: 1: usp_ipc_client_open: failed to connect to the server after 5 retries Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_send_msg_to_spu: failed to connect to iked_spu on SPU - Connection refused. Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_ipc_err_handler: status: 1: usp_ipc_client_open: failed to connect to the server after 5 retries Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_send_msg_to_spu: failed to connect to iked_spu on SPU - Connection refused. Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_ipc_err_handler: status: 1: usp_ipc_client_open: failed to connect to the server after 5 retries Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_send_msg_to_spu: failed to connect to iked_spu on SPU - Connection refused. Dec 27 05:58:05 KMD_INTERNAL_ERROR: iked_re_ipc_err_handler: status: 1: usp_ipc_client_open: failed to connect to the server after 5 retries</em>   Yeah, looks suspicious.  Lets restart ipsec-key-management and see if that helps. &ldquo;<em>restart ipsec-key-management&rdquo;.  </em> <strong><em>Note: If this does not work, you may have to drop to the shell and kill it like a unix process.  </em></strong> <em>&ldquo;start shell&rdquo;</em> &ldquo;kill -9 kmd&rdquo; Idle process should now be back to normal.</p>

<p><a href="http://www.forwardingplane.net/wp-content/uploads/2013/02/Screen-Shot-2013-02-02-at-12.24.11-PM.png"><img src="http://www.forwardingplane.net/wp-content/uploads/2013/02/Screen-Shot-2013-02-02-at-12.24.11-PM.png" alt="Screen Shot 2013-02-02 at 12.24.11 PM" /></a></p>

<p>  CPU&rsquo;s were at a &ldquo;normal&rdquo; state in our environment has the idle process near 90% (+/-).  In the future this will be monitored so that this problem does sneak up on us. Now, the right way to remedy this is to disable those services if you don&rsquo;t need them.  If you do not plan to terminate VPN tunnels, there is no reason to run the services (on by default) to do so.  We opted to both disable and do a more inclusive loopback filter which seems to have taken care of the problem. Take aways from this is multi faceted.  The SRX is a weird beast. It&rsquo;s JunOS, so the inclination is to treat it like a router, but it &rsquo;s not.  It&rsquo;s a firewall.  And an IPS. And a router. I&rsquo;m planning to write up an &ldquo;SRX command cheat sheet&rdquo; for this because it&rsquo;s got enough different pieces and commands that I believe it warrants one.  Secondly, there are a lot of intricacies in monitoring these devices since they arent just routers.  I&rsquo;m hoping to come up with a best practice for monitoring an SRX cluster in a carrier type environment as well.  I&rsquo;m sure they&rsquo;ll both be living documents.</p>
 </div>
    <footer class="post-footer">

  <div class="post-footer-data">
    
<div class="tags">
    
      <div class="tag">
        <a href="https://forwardingplane.net/tags/routing">#Routing</a>
      </div>
    
      <div class="tag">
        <a href="https://forwardingplane.net/tags/security">#Security</a>
      </div>
    
</div>

    <div class="date"> 4 Feb 2013 </div>
  </div>

</footer>


  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "forwwardingplane" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


</article>

  <footer>

  <div class="social-links-footer">

  
  <a href="mailto:my%20lastname%20at%20forwardingplane%20dot%20net"><div class="social-link">Email</div></a>
  

  
  <a href="https://github.com/buraglio" target="_blank"><div class="social-link">GitHub</div></a>
  

  

  
  <a href="https://twitter.com/forwardingplane" target="_blank"><div class="social-link">Twitter</div></a>
  

  
  <a href="https://www.linkedin.com/in/buraglio" target="_blank"><div class="social-link">LinkedIn</div></a>
  

  <div class="social-link">
  <a href="https://forwardingplane.net/index.xml" target="_blank">RSS</a>
  </div>

</div>


  <div class="copyright"> Copyright (c) 2019, all rights reserved. </div>

  <div class="poweredby">
    Powered by <a href="https://gohugo.io/">Hugo</a>.
  </div>

  </footer>

</div> 

</body>
</html>

